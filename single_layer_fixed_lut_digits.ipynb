{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPaiHh/xirl9U3GjplXq39A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mar7i4ka/Lin_Reg/blob/main/single_layer_fixed_lut_digits.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dBhCb-7op8zf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 1. Fixed-Point ⇄ Float Conversion Helpers\n",
        "# ==========================================\n",
        "\n",
        "TOTAL_BITS = 8     # total bits for signed fixed-point (including sign)\n",
        "FRAC_BITS  = 4     # number of fractional bits (so Q3.4)"
      ],
      "metadata": {
        "id": "F3NnK1_pqIib"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def float_to_fixed(x, total_bits=TOTAL_BITS, frac_bits=FRAC_BITS):\n",
        "    \"\"\"\n",
        "    Convert a float x into an 8-bit signed fixed-point integer (Q3.4).\n",
        "    \"\"\"\n",
        "    scale   = 1 << frac_bits                  # 2^frac_bits\n",
        "    max_int = (1 << (total_bits - 1)) - 1     # +127 for 8-bit\n",
        "    min_int = - (1 << (total_bits - 1))       # -128 for 8-bit\n",
        "    xi      = int(np.round(x * scale))\n",
        "    xi_clipped = max(min(xi, max_int), min_int)\n",
        "    return np.int32(xi_clipped)"
      ],
      "metadata": {
        "id": "fDVcUIcpqMfj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fixed_to_float(x_fp, frac_bits=FRAC_BITS):\n",
        "    \"\"\"\n",
        "    Convert an int32 in Q3.4 back to a Python float.\n",
        "    \"\"\"\n",
        "    return float(x_fp) / (1 << frac_bits)"
      ],
      "metadata": {
        "id": "dKXtFO6gqPND"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. Build 256-Entry Sigmoid LUT (Q3.4)\n",
        "# ==========================================\n",
        "\n",
        "LUT_SIZE = 256\n",
        "LUT_MIN  = -6.0\n",
        "LUT_MAX  = +6.0\n",
        "\n",
        "# Sample 256 points in [LUT_MIN, LUT_MAX]\n",
        "lut_x = np.linspace(LUT_MIN, LUT_MAX, LUT_SIZE)\n",
        "# Compute float sigmoid at those points\n",
        "lut_yf = 1.0 / (1.0 + np.exp(-lut_x))\n",
        "# Quantize each sigmoid output to Q3.4\n",
        "lut_fixed = np.array([float_to_fixed(v) for v in lut_yf], dtype=np.int32)\n",
        "\n",
        "def sigmoid_lut(z_fp):\n",
        "    \"\"\"\n",
        "    Given a signed fixed-point input z_fp (Q3.4),\n",
        "    return sigmoid(z_fp) in Q3.4 by LUT lookup.\n",
        "    \"\"\"\n",
        "    z_f = fixed_to_float(z_fp)\n",
        "    idx = int(np.round((z_f - LUT_MIN) * (LUT_SIZE - 1) / (LUT_MAX - LUT_MIN)))\n",
        "    idx = max(0, min(LUT_SIZE - 1, idx))\n",
        "    return lut_fixed[idx]  # still Q3.4"
      ],
      "metadata": {
        "id": "0GXdpjKMqRjP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 3. Load & Preprocess “digits” Dataset\n",
        "# ==========================================\n",
        "\n",
        "digits = load_digits()\n",
        "X = digits.data / 16.0      # original pixels ∈ {0,…,16} → normalize to [0,1]\n",
        "y = digits.target           # integers 0–9\n",
        "\n",
        "# 80% train / 20% test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "tcOZYcxDqXVU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 4. Train Float “Oracle” Single-Layer Model\n",
        "# ==========================================\n",
        "# This is exactly one matrix (10x64) + 10 biases (OvR logistic regression).\n",
        "clf = LogisticRegression(\n",
        "    solver='liblinear',\n",
        "    multi_class='ovr',\n",
        "    max_iter=1000,\n",
        "    random_state=42\n",
        ")\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Extract float weights & biases:\n",
        "#   Wf shape = (10, 64), bf shape = (10,)\n",
        "Wf = clf.coef_.astype(np.float32)\n",
        "bf = clf.intercept_.astype(np.float32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AXyr_BLqcEo",
        "outputId": "10ec9af8-ceab-404d-b3aa-9c5ded3171f3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 5. Quantize Weights & Biases to Q3.4\n",
        "# ==========================================\n",
        "Wq = np.vectorize(lambda v: float_to_fixed(v))(Wf)  # int32 → 10×64 (Q3.4)\n",
        "bq = np.vectorize(lambda v: float_to_fixed(v))(bf)  # int32 → length 10\n"
      ],
      "metadata": {
        "id": "Ws4aZxV-qqSM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Define Fixed-Point Single-Layer Forward\n",
        "# ==========================================\n",
        "def forward_one_layer_fp(x_fp, W_fp, b_fp):\n",
        "    \"\"\"\n",
        "    Compute one single-layer network in pure fixed-point + LUT:\n",
        "      x_fp:  int32 array of length 64 (input in Q3.4)\n",
        "      W_fp:  int32 array shape (10, 64) (fixed-point weights Q3.4)\n",
        "      b_fp:  int32 array length 10 (biases Q3.4)\n",
        "    Returns a length-10 float array (dequantized sigmoid outputs).\n",
        "    \"\"\"\n",
        "    out_fp = np.zeros(10, dtype=np.int32)\n",
        "\n",
        "    # For each of the 10 output neurons:\n",
        "    for c in range(10):\n",
        "        acc = np.int32(0)\n",
        "        # Multiply-accumulate over 64 inputs (all Q3.4)\n",
        "        for i in range(64):\n",
        "            prod = np.int32(x_fp[i]) * np.int32(W_fp[c, i])  # Q7.8 intermediate\n",
        "            acc  = np.int32(acc + (prod >> FRAC_BITS))        # shift back to Q3.4\n",
        "\n",
        "        acc = np.int32(acc + b_fp[c])  # add bias (Q3.4)\n",
        "\n",
        "        # Activation via LUT (still Q3.4)\n",
        "        out_fp[c] = sigmoid_lut(acc)\n",
        "\n",
        "    # Dequantize each output to float\n",
        "    return np.array([fixed_to_float(v) for v in out_fp])\n"
      ],
      "metadata": {
        "id": "0mR29P78qtWt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 7. Verification on Test Set\n",
        "# ==========================================\n",
        "correct = 0\n",
        "N_test  = len(X_test)\n",
        "\n",
        "for idx in range(N_test):\n",
        "    x_f  = X_test[idx].astype(np.float32)\n",
        "    # 7.1. Quantize the 64-D input vector to Q3.4\n",
        "    x_fp = np.vectorize(lambda v: float_to_fixed(v))(x_f)\n",
        "\n",
        "    # 7.2. Run fixed-point forward pass\n",
        "    y_pred_fp = forward_one_layer_fp(x_fp, Wq, bq)  # length-10 float\n",
        "\n",
        "    pred = int(np.argmax(y_pred_fp))  # pick the highest sigmoid\n",
        "    if pred == y_test[idx]:\n",
        "        correct += 1\n",
        "\n",
        "accuracy = correct / N_test * 100\n",
        "print(f\"Single-Layer (Fixed-Point+LUT) Accuracy on “digits”: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDZ8mJPGqxZM",
        "outputId": "653ba9ef-094d-4c04-f7fc-102eb7882131"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single-Layer (Fixed-Point+LUT) Accuracy on “digits”: 95.83%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 8. (Optional) Compare to Float Outputs on a Few Samples\n",
        "# ==========================================\n",
        "print(\"\\nA few sample comparisons (float vs. fixed-point):\")\n",
        "for i in range(5):\n",
        "    x_f = X_test[i].astype(np.float32)\n",
        "    y_float = clf.predict_proba(x_f.reshape(1, -1))[0]  # float probabilities\n",
        "    # Note: clfs.predict_proba applies softmax, but for OvR/logistic we can treat\n",
        "    #       the raw sigmoid outputs (as implemented above) similarly. We'll just call\n",
        "    #       forward_one_layer_fp and compare its 10-vector to the float logistic odds.\n",
        "    x_fp    = np.vectorize(lambda v: float_to_fixed(v))(x_f)\n",
        "    y_fixed = forward_one_layer_fp(x_fp, Wq, bq)\n",
        "\n",
        "    print(f\"Sample {i+1}: true={y_test[i]}\")\n",
        "    print(f\"  float-proba  = {[round(p, 3) for p in y_float]}\")\n",
        "    print(f\"  fixed-sigmoid = {[round(v, 3) for v in y_fixed]}\")\n",
        "    print(f\"  argmax_f={int(np.argmax(y_float))}, argmax_fp={int(np.argmax(y_fixed))}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kz98vUnGq1s5",
        "outputId": "9342e307-d39d-4415-8017-ae38f3877804"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "A few sample comparisons (float vs. fixed-point):\n",
            "Sample 1: true=6\n",
            "  float-proba  = [np.float64(0.004), np.float64(0.002), np.float64(0.0), np.float64(0.0), np.float64(0.003), np.float64(0.004), np.float64(0.969), np.float64(0.001), np.float64(0.011), np.float64(0.005)]\n",
            "  fixed-sigmoid = [np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(1.0), np.float64(0.0), np.float64(0.0), np.float64(0.0)]\n",
            "  argmax_f=6, argmax_fp=6\n",
            "\n",
            "Sample 2: true=9\n",
            "  float-proba  = [np.float64(0.008), np.float64(0.0), np.float64(0.0), np.float64(0.007), np.float64(0.004), np.float64(0.127), np.float64(0.0), np.float64(0.001), np.float64(0.006), np.float64(0.848)]\n",
            "  fixed-sigmoid = [np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.062), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.75)]\n",
            "  argmax_f=9, argmax_fp=9\n",
            "\n",
            "Sample 3: true=3\n",
            "  float-proba  = [np.float64(0.0), np.float64(0.001), np.float64(0.003), np.float64(0.909), np.float64(0.0), np.float64(0.027), np.float64(0.001), np.float64(0.002), np.float64(0.033), np.float64(0.024)]\n",
            "  fixed-sigmoid = [np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(1.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0)]\n",
            "  argmax_f=3, argmax_fp=3\n",
            "\n",
            "Sample 4: true=7\n",
            "  float-proba  = [np.float64(0.016), np.float64(0.022), np.float64(0.002), np.float64(0.001), np.float64(0.007), np.float64(0.073), np.float64(0.0), np.float64(0.775), np.float64(0.003), np.float64(0.1)]\n",
            "  fixed-sigmoid = [np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.438), np.float64(0.0), np.float64(0.062)]\n",
            "  argmax_f=7, argmax_fp=7\n",
            "\n",
            "Sample 5: true=2\n",
            "  float-proba  = [np.float64(0.002), np.float64(0.004), np.float64(0.827), np.float64(0.112), np.float64(0.0), np.float64(0.006), np.float64(0.015), np.float64(0.0), np.float64(0.032), np.float64(0.001)]\n",
            "  fixed-sigmoid = [np.float64(0.0), np.float64(0.0), np.float64(0.5), np.float64(0.062), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0)]\n",
            "  argmax_f=2, argmax_fp=2\n",
            "\n"
          ]
        }
      ]
    }
  ]
}