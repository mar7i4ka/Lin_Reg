{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.16",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 44744,
          "sourceType": "datasetVersion",
          "datasetId": 33672
        },
        {
          "sourceId": 370089,
          "sourceType": "datasetVersion",
          "datasetId": 902
        },
        {
          "sourceId": 1747344,
          "sourceType": "datasetVersion",
          "datasetId": 772636
        },
        {
          "sourceId": 2344503,
          "sourceType": "datasetVersion",
          "datasetId": 1415343
        },
        {
          "sourceId": 10611214,
          "sourceType": "datasetVersion",
          "datasetId": 6420200
        }
      ],
      "dockerImageVersionId": 30822,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "üìúComplete Credit Risk Modeling üîç | 1. EDA",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mar7i4ka/Lin_Reg/blob/main/%F0%9F%93%9CComplete_Credit_Risk_Modeling_%F0%9F%94%8D_%7C_1_EDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "husainsb_lendingclub_issued_loans_path = kagglehub.dataset_download('husainsb/lendingclub-issued-loans')\n",
        "wordsforthewise_lending_club_path = kagglehub.dataset_download('wordsforthewise/lending-club')\n",
        "ethon0426_lending_club_20072020q1_path = kagglehub.dataset_download('ethon0426/lending-club-20072020q1')\n",
        "adarshsng_lending_club_loan_data_csv_path = kagglehub.dataset_download('adarshsng/lending-club-loan-data-csv')\n",
        "beatafaron_loan_credit_risk_and_population_stability_path = kagglehub.dataset_download('beatafaron/loan-credit-risk-and-population-stability')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "KT_TTS413tzi"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "> In this kernel I will gradually create a full credit risk estimation model compliant with Basel II Standards.\n",
        "> If you are interested in this topic please Vote for this notebook üòäüëç"
      ],
      "metadata": {
        "id": "KjTydQS-3tzj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìò SCOPE\n",
        "\n",
        "> You have just been hired as a data scientist working in a team of credit risk management in LendingClub Company - a US peer-to-peer lending company, headquartered in San Francisco, California. It is the first peer-to-peer lender to register its offerings as securities with the Securities and Exchange Commission (SEC), and to offer loan trading on a secondary market. `LendingClub` is the world's largest peer-to-peer lending platform.\n",
        "> You got a data that contains the information about past loan applicants and whether they ‚Äòdefaulted‚Äô or not. Here what You should do:\n",
        "\n",
        "## üéØBusiness Tasks\n",
        "1. Build and implement the best possible credit risk model - calculating Propability of Default (PD)\n",
        "2. Use Weight of Evidence and Information Value for categorical variables\n",
        "3. Build the behavioral score card based on selected features\n",
        "4. Monitor Population stability - script for monitoring the model in the future\n",
        "\n",
        "\n",
        "### ‚ÑπÔ∏èShort info:\n",
        "> (PD) Propability of Default  - Credit Risk -  the probability that someone who borrowed money, does not repay their debt.\n",
        " Measuring credit risk well is the most effective way of ensuring that credit losses would be minimized.\n",
        "The lender can also require collaterals. Thus, if a borrower does not repay, the asset that serves as a collateral can be acquired by the lender and sold to a third party in order to cover some of the losses in case the borrower defaults. Third, the lender can make sure that the greater loss they expect, the higher the price of lending that they charge.\n",
        "> (PSI) Population Stability Index - compares the distribution of predicted probability in scoring data with predicted probability in training data.\n",
        "\n",
        "## üîçUnderstanding the business process\n",
        "> When the company receives a loan application, the company has to make a decision for loan approval based on the applicant‚Äôs profile. Two types of risks are associated with the bank‚Äôs decision:\n",
        "\n",
        "> - If the applicant is likely to repay the loan, then not approving the loan results in a loss of business to the company\n",
        "> - If the applicant is not likely to repay the loan (likely to default), then approving the loan may lead to a financial loss for the company\n",
        "\n",
        "## üìú Basel II Standards\n",
        ">Basel II is an international business standard that requires financial institutions to maintain enough cash reserves to cover risks incurred by their operations. The Basel accords are a series of recommendations on banking laws and regulations issued by the Basel Committee on Banking Supervision.\n",
        "The three pillars of the Basel II Accord are minimal capital requirements (SA - Standardized Approach, F-IRB - foundation Intrnal Ratings Based Approach, A-IRB- Internal Rating Based Approach) supervisory review, market discipline.\n",
        "Under the standardized approach, the amount that should be held as capital for every retail exposure, as a percentage of the total exposure is 75%.\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "id": "1cfPQ_0i3tzk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚öôÔ∏èEDA + Feature Engineering"
      ],
      "metadata": {
        "id": "WwwLSRJr3tzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.options.display.max_columns = None"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-27T09:15:49.718572Z",
          "iopub.execute_input": "2025-02-27T09:15:49.718819Z",
          "iopub.status.idle": "2025-02-27T09:15:52.874526Z",
          "shell.execute_reply.started": "2025-02-27T09:15:49.718795Z",
          "shell.execute_reply": "2025-02-27T09:15:52.873382Z"
        },
        "id": "M0gH5UkP3tzl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Reading file"
      ],
      "metadata": {
        "id": "RErtn20N3tzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/kaggle/input/loan-credit-risk-and-population-stability/loan_2014_18.csv')\n",
        "# dictionary=pd.read_excel('/kaggle/input/loan-credit-risk-and-population-stability/LoanDataDictionary.xlsx')\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-27T09:15:52.875462Z",
          "iopub.execute_input": "2025-02-27T09:15:52.875788Z",
          "iopub.status.idle": "2025-02-27T09:16:48.528715Z",
          "shell.execute_reply.started": "2025-02-27T09:15:52.875762Z",
          "shell.execute_reply": "2025-02-27T09:16:48.527693Z"
        },
        "id": "fCJMGnu43tzm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(df.columns[0], axis=1)\n",
        "df.shape"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-27T09:16:48.529854Z",
          "iopub.execute_input": "2025-02-27T09:16:48.530089Z",
          "iopub.status.idle": "2025-02-27T09:16:49.873107Z",
          "shell.execute_reply.started": "2025-02-27T09:16:48.53006Z",
          "shell.execute_reply": "2025-02-27T09:16:49.871757Z"
        },
        "id": "m9G3vAaV3tzm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Identifying the target (y)\n",
        "> dependent variable :\n",
        "In our dataset the dependent variable js the \"loan_status\" column.\n",
        "First we check what types of categories it can take, then map it values as follows: </br>\n",
        ">'Fully Paid', 'Current', 'In Grace Period'  - are map as '1' - **positive** </br>\n",
        ">'Late (16-30 days)', 'Late (31-120 days)', 'Charged Off', 'Default' - are map with '0' - **negative**\n"
      ],
      "metadata": {
        "id": "f78veYMk3tzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the mapping\n",
        "loan_status_mapping = {\n",
        "    'Fully Paid': 1,\n",
        "    'Current': 1,\n",
        "    'In Grace Period': 1,\n",
        "    'Late (16-30 days)': 0,\n",
        "    'Late (31-120 days)': 0,\n",
        "    'Charged Off': 0,\n",
        "    'Default': 0\n",
        "}\n",
        "\n",
        "# Apply the mapping to the 'Loan_Status' column\n",
        "df['loan_status_binary'] = df['loan_status'].map(loan_status_mapping)\n",
        "df.drop('loan_status', axis=1, inplace=True)\n",
        "\n",
        "df['loan_status_binary'].value_counts()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-27T09:16:49.874066Z",
          "iopub.execute_input": "2025-02-27T09:16:49.874314Z",
          "iopub.status.idle": "2025-02-27T09:16:51.509282Z",
          "shell.execute_reply.started": "2025-02-27T09:16:49.874291Z",
          "shell.execute_reply": "2025-02-27T09:16:51.508269Z"
        },
        "id": "3iU2L60Z3tzm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Missing values\n",
        "> Check the missing values. columns that are missing by more than 50 percent will be removed."
      ],
      "metadata": {
        "id": "IxpE-OWX3tzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def missing_data_summary(df, threshold=0):\n",
        "    \"\"\"\n",
        "    Summarizes missing data, showing count and percentage of missing values for each column.\n",
        "    Filters columns based on a missing percentage threshold.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): The dataframe to analyze.\n",
        "        threshold (float): The minimum percentage of missing data to include in the summary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A summary of missing data.\n",
        "    \"\"\"\n",
        "    return (pd.DataFrame(df.isna().sum())\n",
        "            .reset_index()\n",
        "            .rename(columns={'index': 'Column', 0: 'mis_count'})\n",
        "            .query('mis_count > 0')  # Only include columns with missing values\n",
        "            .assign(Missing_Percentage=lambda x: x['mis_count'] / df.shape[0] * 100)\n",
        "            .query(f'Missing_Percentage > {threshold}')  # Filter by threshold\n",
        "            .sort_values('mis_count', ascending=False)\n",
        "            .reset_index(drop=True))\n",
        "\n",
        "missing = missing_data_summary(df,51)\n",
        "missing"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-27T09:16:51.510178Z",
          "iopub.execute_input": "2025-02-27T09:16:51.510419Z",
          "iopub.status.idle": "2025-02-27T09:16:53.954184Z",
          "shell.execute_reply.started": "2025-02-27T09:16:51.510394Z",
          "shell.execute_reply": "2025-02-27T09:16:53.953217Z"
        },
        "id": "IF8WpPbc3tzn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the first column of 'missing' to a list\n",
        "columns_to_drop = missing.iloc[:, 0].tolist()\n",
        "\n",
        "# Drop these columns from the DataFrame 'df'\n",
        "df = df.drop(columns=columns_to_drop, errors='ignore')  # 'errors=\"ignore\"' ensures no error if a column is missing"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-27T09:16:53.955287Z",
          "iopub.execute_input": "2025-02-27T09:16:53.955514Z",
          "iopub.status.idle": "2025-02-27T09:16:54.913526Z",
          "shell.execute_reply.started": "2025-02-27T09:16:53.955492Z",
          "shell.execute_reply": "2025-02-27T09:16:54.912202Z"
        },
        "id": "ILbhnjPG3tzn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Categorical Features"
      ],
      "metadata": {
        "id": "VEtu24xz3tzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-27T09:16:54.914547Z",
          "iopub.execute_input": "2025-02-27T09:16:54.914919Z",
          "iopub.status.idle": "2025-02-27T09:16:54.927769Z",
          "shell.execute_reply.started": "2025-02-27T09:16:54.914891Z",
          "shell.execute_reply": "2025-02-27T09:16:54.926609Z"
        },
        "id": "Sl7-QsNc3tzn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objects\n",
        "> First let's check the 'object' columns. We see that there are some columns that can be transformed to date or numeric columns."
      ],
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "Hyif5jb53tzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.select_dtypes(include=['object']).head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-27T09:16:54.928776Z",
          "iopub.execute_input": "2025-02-27T09:16:54.929064Z",
          "iopub.status.idle": "2025-02-27T09:16:55.378504Z",
          "shell.execute_reply.started": "2025-02-27T09:16:54.929037Z",
          "shell.execute_reply": "2025-02-27T09:16:55.377404Z"
        },
        "id": "j0cqzUgi3tzn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove leading/trailing whitespaces from objects\n",
        "df = df.apply(lambda col: col.str.strip() if col.dtypes == 'object' else col)\n",
        "\n",
        "# convert dates to 'datetime' types\n",
        "df['issue_d'] = pd.to_datetime(df['issue_d'], format='%Y-%m-%d')\n",
        "df['earliest_cr_line'] = pd.to_datetime(df['earliest_cr_line'], format='%b-%Y')\n",
        "df['last_pymnt_d'] = pd.to_datetime(df['last_pymnt_d'], format='%b-%Y')\n",
        "df['last_credit_pull_d'] = pd.to_datetime(df['last_credit_pull_d'], format='%b-%Y')\n",
        "\n",
        "# int_rate and revol_util for int type\n",
        "df['int_rate%']= pd.to_numeric(df['int_rate'].str.strip('%'))\n",
        "df['revol_util%'] =pd.to_numeric(df['revol_util'].str.strip('%'))\n",
        "\n",
        "columns_to_drop = {'title','zip_code','pymnt_plan', 'emp_title','int_rate','revol_util', 'url'}\n",
        "df.drop(columns_to_drop, axis = 1, inplace = True)\n",
        "\n",
        "df.debt_settlement_flag = np.where(df.debt_settlement_flag == 'Y',1,0)\n",
        "\n",
        "df.term_36_months = np.where(df.term == '36 months',1,0)\n",
        "df.drop('term', axis=1, inplace=True)\n",
        "\n",
        "df['emp_length'] = df['emp_length'].fillna('')\n",
        "df['emp_length'] = pd.to_numeric(df['emp_length'].str.replace('<', '', regex=False).str[:2].str.strip(), errors='coerce')\n",
        "\n",
        "df.hardship_flag.fillna('N', inplace=True) #it looks like the Nans are as strings so\n",
        "df.hardship_flag = np.where( df.hardship_flag == 'NaN','N', df.hardship_flag )\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-27T09:16:55.379563Z",
          "iopub.execute_input": "2025-02-27T09:16:55.379919Z",
          "iopub.status.idle": "2025-02-27T09:17:13.399507Z",
          "shell.execute_reply.started": "2025-02-27T09:16:55.379892Z",
          "shell.execute_reply": "2025-02-27T09:17:13.398071Z"
        },
        "id": "JYSHZTej3tzn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sorted_unique_counts(df):\n",
        "    # Select columns with categorical data\n",
        "    object_cols = df.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "    # Get the number of unique entries in each categorical column\n",
        "    object_nunique = {col: df[col].nunique() for col in object_cols}\n",
        "\n",
        "    # Convert to DataFrame and sort by unique counts\n",
        "    unique_counts = pd.DataFrame(list(object_nunique.items()), columns=['Column', 'Unique Count'])\n",
        "    unique_counts = unique_counts.sort_values(by='Unique Count').reset_index(drop=True)\n",
        "\n",
        "    return unique_counts\n",
        "\n",
        "get_sorted_unique_counts(df)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-27T09:17:13.400507Z",
          "iopub.execute_input": "2025-02-27T09:17:13.400805Z",
          "iopub.status.idle": "2025-02-27T09:17:14.455657Z",
          "shell.execute_reply.started": "2025-02-27T09:17:13.400781Z",
          "shell.execute_reply": "2025-02-27T09:17:14.454386Z"
        },
        "id": "yY7-rhBN3tzn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Check how many unique values are in object columns. We check if binning is needed.\n",
        "Check all for any inconsistences or imbalances"
      ],
      "metadata": {
        "id": "TAw5a0t73tzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def value_counter(df):\n",
        "    value_series = pd.DataFrame()\n",
        "    for col in df.select_dtypes([\"object\"]).columns:\n",
        "        print(df[col].value_counts(dropna = False))\n",
        "\n",
        "value_counter(df)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-27T09:17:14.45658Z",
          "iopub.execute_input": "2025-02-27T09:17:14.45682Z",
          "iopub.status.idle": "2025-02-27T09:17:15.235043Z",
          "shell.execute_reply.started": "2025-02-27T09:17:14.456798Z",
          "shell.execute_reply": "2025-02-27T09:17:15.233542Z"
        },
        "id": "S8RPz-7u3tzo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üí°üîßKey observations\n",
        "\n",
        "> sub_grade : is already grupped by grade\n",
        ">\n",
        "> home_wnership : The majority categories (mortgage and rent) dominate the data, while own and especially other are underrepresented.\n",
        "Such an imbalance can lead to biased models, where predictions are skewed toward the majority classes.\n",
        ">\n",
        "> purpose : here is huge imbalance, especialy on medical, car, small_business, vacation, moving, house, renewable_energy, wedding, educational. It should be grouped\n",
        ">\n",
        "> addr_state : as well"
      ],
      "metadata": {
        "id": "Nmk5HiA43tzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop('sub_grade', axis=1, inplace = True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-27T09:17:15.235876Z",
          "iopub.execute_input": "2025-02-27T09:17:15.23614Z",
          "iopub.status.idle": "2025-02-27T09:17:15.884735Z",
          "shell.execute_reply.started": "2025-02-27T09:17:15.236115Z",
          "shell.execute_reply": "2025-02-27T09:17:15.882758Z"
        },
        "id": "p_YCm0Ug3tzo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîçUsing WoE & IV for categorical features\n",
        ">Now we will group categorical data based on Weight of Evidence (WoE) and create a table with relevant measures.\n",
        "> **WoE**: WoE is computed as:<br>\n",
        "> \"Non-Event\" - bad = '0' defaulted loans, <br>\n",
        "> \"Event\"     - good = '1' non-default loans.<br>\n",
        "\n",
        "$$\n",
        "\\text{WoE} = \\ln\\left(\\frac{\\text{Proportion of Non-Event}}{\\text{Proportion of Event}}\\right)\n",
        "$$\n",
        "\n",
        "\n",
        "> **IV (Information Value)** indicates the predictive power of the categorical variable. Total IV: Interpret the IV to understand the variable's predictive strength.\n",
        "\n",
        "$$\n",
        "\\text{IV} = \\sum \\left( (\\text{Proportion of Non-Event} - \\text{Proportion of Event}) \\cdot \\text{WoE} \\right)\n",
        "$$\n",
        "\n",
        ">Interpretation of IV:<br>\n",
        "> < 0.02: Not Predictive <br>\n",
        "> 0.02 - 0.1: Weak Predictive Power<br>\n",
        "> 0.1 - 0.3: Medium Predictive Power<br>\n",
        "> 0.3+: Strong Predictive Power<br>\n",
        "> IV > 0.5: Suspiciously strong ‚Äî Could indicate overfitting or data issues."
      ],
      "metadata": {
        "id": "fa7Ihwbx3tzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_categorical_columns(df, categorical_cols, target_col, woe_table=None, iv_summary=None, append=False):\n",
        "    \"\"\"\n",
        "    Calculate WoE and IV for given categorical columns, including NaNs as a separate category.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Input DataFrame.\n",
        "        categorical_cols (list): List of categorical column names to process.\n",
        "        target_col (str): Name of the binary target column (1 = Good, 0 = Bad).\n",
        "        woe_table (pd.DataFrame): Existing WoE table (required if append=True).\n",
        "        iv_summary (pd.DataFrame): Existing IV summary (required if append=True).\n",
        "        append (bool): If True, append results to existing tables.\n",
        "\n",
        "    Returns:\n",
        "        woe_table (pd.DataFrame): WoE table with all processed columns.\n",
        "        iv_summary (pd.DataFrame): IV summary table with all processed columns.\n",
        "    \"\"\"\n",
        "    if append and (woe_table is None or iv_summary is None):\n",
        "        raise ValueError(\"Existing tables must be provided when append=True.\")\n",
        "\n",
        "    # Initialize results if not appending\n",
        "    if not append:\n",
        "        woe_table = pd.DataFrame()\n",
        "        iv_summary = pd.DataFrame()\n",
        "\n",
        "    new_woe_results = []\n",
        "    new_summary = []\n",
        "\n",
        "    # Process each categorical column\n",
        "    for col in categorical_cols:\n",
        "        # Handle NaN values by creating a separate 'NaN' category\n",
        "        df[col] = df[col].fillna('NaN')\n",
        "\n",
        "        # Group by the column to calculate metrics\n",
        "        stats = df.groupby(col).agg(\n",
        "            event_count=(target_col, 'sum'),\n",
        "            total_count=(target_col, 'count')\n",
        "        ).reset_index()\n",
        "\n",
        "        # Calculate non-event count and proportions\n",
        "        stats['non_event_count'] = stats['total_count'] - stats['event_count']\n",
        "        total_events = stats['event_count'].sum()\n",
        "        total_non_events = stats['non_event_count'].sum()\n",
        "\n",
        "        # Avoid division by zero\n",
        "        stats['event_rate'] = stats['event_count'] / (total_events + 1e-6)\n",
        "        stats['non_event_rate'] = stats['non_event_count'] / (total_non_events + 1e-6)\n",
        "\n",
        "        # Calculate WoE\n",
        "        stats['woe'] = np.log((stats['non_event_rate'] + 1e-6) / (stats['event_rate'] + 1e-6))\n",
        "\n",
        "        # Calculate IV for each category\n",
        "        stats['iv'] = (stats['non_event_rate'] - stats['event_rate']) * stats['woe']\n",
        "\n",
        "        # Compute total IV for the column\n",
        "        total_iv = stats['iv'].sum()\n",
        "\n",
        "        # Add column name and rename for desired format\n",
        "        stats['name'] = col  # Add column name\n",
        "        stats.rename(columns={col: 'sub_name'}, inplace=True)  # Rename column to 'sub_name'\n",
        "\n",
        "        # Reorder columns\n",
        "        stats = stats[['name', 'sub_name', 'event_count', 'total_count', 'non_event_count',\n",
        "                       'event_rate', 'non_event_rate', 'woe', 'iv']]\n",
        "\n",
        "        # Append results\n",
        "        new_woe_results.append(stats)\n",
        "        new_summary.append({'Column': col, 'IV': total_iv})\n",
        "\n",
        "    # Combine results with existing tables if appending\n",
        "    new_woe_table = pd.concat(new_woe_results, ignore_index=True)\n",
        "    new_iv_summary = pd.DataFrame(new_summary).sort_values(by='IV', ascending=False)\n",
        "\n",
        "    if append:\n",
        "        woe_table = pd.concat([woe_table, new_woe_table], ignore_index=True)\n",
        "        iv_summary = pd.concat([iv_summary, new_iv_summary], ignore_index=True).sort_values(by='IV', ascending=False)\n",
        "    else:\n",
        "        woe_table = new_woe_table\n",
        "        iv_summary = new_iv_summary\n",
        "\n",
        "    return woe_table, iv_summary"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-27T09:17:15.885599Z",
          "iopub.execute_input": "2025-02-27T09:17:15.885849Z",
          "iopub.status.idle": "2025-02-27T09:17:15.896568Z",
          "shell.execute_reply.started": "2025-02-27T09:17:15.885825Z",
          "shell.execute_reply": "2025-02-27T09:17:15.895356Z"
        },
        "id": "M5mXF4kd3tzo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial processing\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "woe_table, iv_summary = process_categorical_columns(df, categorical_cols, 'loan_status_binary')\n",
        "\n",
        "# Save initial results\n",
        "# woe_table.to_csv('woe_table.csv', index=False)\n",
        "# iv_summary.to_csv('iv_summary.csv', index=False)\n",
        "\n",
        "iv_summary"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-27T09:17:15.897557Z",
          "iopub.execute_input": "2025-02-27T09:17:15.897837Z",
          "iopub.status.idle": "2025-02-27T09:17:18.210238Z",
          "shell.execute_reply.started": "2025-02-27T09:17:15.897802Z",
          "shell.execute_reply": "2025-02-27T09:17:18.209128Z"
        },
        "id": "zHph-lyq3tzo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        " #dropping where IV < 0.02\n",
        "drop= {'application_type','initial_list_status','addr_state','purpose'}\n",
        "df.drop(drop, axis=1, inplace=True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-27T09:17:18.21129Z",
          "iopub.execute_input": "2025-02-27T09:17:18.211525Z",
          "iopub.status.idle": "2025-02-27T09:17:18.806707Z",
          "shell.execute_reply.started": "2025-02-27T09:17:18.211503Z",
          "shell.execute_reply": "2025-02-27T09:17:18.805388Z"
        },
        "id": "bUF9nJeq3tzo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Initial WoE Table:\")\n",
        "woe_table[woe_table.name == 'hardship_flag']"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-27T09:17:18.807573Z",
          "iopub.execute_input": "2025-02-27T09:17:18.807982Z",
          "iopub.status.idle": "2025-02-27T09:17:18.820298Z",
          "shell.execute_reply.started": "2025-02-27T09:17:18.807945Z",
          "shell.execute_reply": "2025-02-27T09:17:18.819045Z"
        },
        "id": "1cMNBTOh3tzo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Initial WoE Table:\")\n",
        "woe_table[woe_table.name == 'verification_status']"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-27T09:17:18.821325Z",
          "iopub.execute_input": "2025-02-27T09:17:18.821591Z",
          "iopub.status.idle": "2025-02-27T09:17:18.837875Z",
          "shell.execute_reply.started": "2025-02-27T09:17:18.821566Z",
          "shell.execute_reply": "2025-02-27T09:17:18.837097Z"
        },
        "id": "uHW0Aabz3tzo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Initial WoE Table:\")\n",
        "woe_table[woe_table.name == 'home_ownership']"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-27T09:17:18.839178Z",
          "iopub.execute_input": "2025-02-27T09:17:18.839466Z",
          "iopub.status.idle": "2025-02-27T09:17:18.854726Z",
          "shell.execute_reply.started": "2025-02-27T09:17:18.839419Z",
          "shell.execute_reply": "2025-02-27T09:17:18.853074Z"
        },
        "id": "6N7rgUJs3tzp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "> As we see the WoE, IV and total counts numbers we can group the 'ANY', 'MORTGAGE' and 'NONE' together."
      ],
      "metadata": {
        "id": "WZ0jQxdq3tzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "home_ownership_mapping = {\n",
        "    'ANY' : 'other',\n",
        "    'MORTGAGE' : 'other',\n",
        "    'RENT' : 'rent',\n",
        "    'OWN' : 'own',\n",
        "    'NONE' : 'other'\n",
        "}\n",
        "df.home_ownership = df.home_ownership.map(home_ownership_mapping)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-27T09:17:18.855587Z",
          "iopub.execute_input": "2025-02-27T09:17:18.855821Z",
          "iopub.status.idle": "2025-02-27T09:17:18.998041Z",
          "shell.execute_reply.started": "2025-02-27T09:17:18.855798Z",
          "shell.execute_reply": "2025-02-27T09:17:18.996955Z"
        },
        "id": "BofgVMXW3tzp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìä Checking categorical features vs target"
      ],
      "metadata": {
        "id": "OWebwJPz3tzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.font_manager as fm\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "\n",
        "# Set global aesthetics for plots\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.dpi'] = 120\n",
        "plt.rcParams['font.size'] = 14\n",
        "#plt.rcParams['font.family'] = 'Roboto Condensed'  # Set font\n",
        "\n",
        "# Ensure font is available\n",
        "font_path = fm.findfont(fm.FontProperties(family='Roboto Condensed'))\n",
        "\n",
        "# Define a custom color palette\n",
        "palette = {1: '#949398FF', 0: '#FC766AFF', 'NaN': '#D3D3D3'}  # Add a color for NaN values\n",
        "\n",
        "# Function to format large numbers as plain text\n",
        "def plain_formatter(x, _):\n",
        "    return f'{int(x):,}'  # Format with commas for thousands\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-02-27T09:49:53.212Z"
        },
        "id": "3i-ZZM3r3tzp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Select object columns (replace this with your actual DataFrame and column selection)\n",
        "object_columns = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Create subplots\n",
        "n_columns = 2  # Number of plots per row\n",
        "n_rows = (len(object_columns) + 1) // n_columns  # Calculate number of rows\n",
        "fig, axes = plt.subplots(n_rows, n_columns, figsize=(16, n_rows * 7))  # Dynamic height\n",
        "\n",
        "# Flatten axes for easier iteration\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Plot each histogram\n",
        "for i, column in enumerate(object_columns):\n",
        "    df[column] = df[column].fillna('NaN')  # Replace NaN with a string 'NaN' for visualization\n",
        "    sns.histplot(\n",
        "        data=df,\n",
        "        x=column,\n",
        "        hue='loan_status_binary',\n",
        "        multiple='stack',\n",
        "        kde=False,\n",
        "        palette=palette,\n",
        "        hue_order=[0, 1, 'NaN'],  # Ensure NaN appears in the legend and the plot\n",
        "        ax=axes[i],\n",
        "        legend=False  # Suppress legends for individual plots\n",
        "    )\n",
        "    # Set column name as title\n",
        "    axes[i].set_title(column, fontsize=18, pad=14)  # Title above each histogram\n",
        "    # Remove x-axis label\n",
        "    axes[i].set_xlabel('')\n",
        "    # Adjust y-axis label\n",
        "    axes[i].set_ylabel('Number of Loans', fontsize=16, labelpad=14)\n",
        "    # Format ticks\n",
        "    axes[i].tick_params(axis='x', rotation=45, labelsize=14)\n",
        "    axes[i].tick_params(axis='y', labelsize=14)\n",
        "    # Format y-axis without scientific notation\n",
        "    axes[i].yaxis.set_major_formatter(FuncFormatter(plain_formatter))\n",
        "\n",
        "# Remove unused subplots\n",
        "for j in range(len(object_columns), len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "# Add a single legend for the custom colors\n",
        "handles = [plt.Line2D([0], [0], color=color, lw=4, label=label)\n",
        "           for label, color in zip(['Defaulted (0)', 'Not Defaulted (1)', 'NaN'], palette.values())]\n",
        "fig.legend(handles=handles, loc='upper center', fontsize=16, title='Loan Status', title_fontsize=18, ncol=3)\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout(rect=[0, 0, 1,0.9])  # Leave space for the legend\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-02-27T09:49:53.212Z"
        },
        "id": "iUPpGuOB3tzp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Date features"
      ],
      "metadata": {
        "id": "3p5CvXdw3tzp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Date columns often contain valuable temporal information that can improve predictive modeling. Lets extract meaningful features from these date columns.\n",
        "\n"
      ],
      "metadata": {
        "id": "sQtJt2DZ3tzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "date_cols = df.select_dtypes(include=['datetime64']).columns\n",
        "df[date_cols].head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-27T09:17:30.286233Z",
          "iopub.execute_input": "2025-02-27T09:17:30.286451Z",
          "iopub.status.idle": "2025-02-27T09:17:30.351335Z",
          "shell.execute_reply.started": "2025-02-27T09:17:30.286429Z",
          "shell.execute_reply": "2025-02-27T09:17:30.350437Z"
        },
        "id": "8T1r_nvi3tzp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df[date_cols].isna().sum()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-27T09:17:30.352223Z",
          "iopub.execute_input": "2025-02-27T09:17:30.35244Z",
          "iopub.status.idle": "2025-02-27T09:17:30.385002Z",
          "shell.execute_reply.started": "2025-02-27T09:17:30.352417Z",
          "shell.execute_reply": "2025-02-27T09:17:30.383807Z"
        },
        "id": "2sl3hraO3tzp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# Assuming 'today' as the current date\n",
        "today = pd.to_datetime(\"today\")\n",
        "\n",
        "#impute a today date to distinguish missing values.\n",
        "df['last_pymnt_d'].fillna(today, inplace=True)\n",
        "df['last_credit_pull_d'].fillna(today, inplace=True)\n",
        "\n",
        "# Time differences\n",
        "df['loan_age'] = (today - df['issue_d']).dt.days\n",
        "df['credit_history_length'] = (df['issue_d'] - df['earliest_cr_line']).dt.days\n",
        "df['time_since_last_payment'] = (today - df['last_pymnt_d']).dt.days\n",
        "df['time_since_last_credit_pull'] = (today - df['last_credit_pull_d']).dt.days\n",
        "\n",
        "# Temporal components\n",
        "df['issue_year'] = df['issue_d'].dt.year\n",
        "df['issue_month'] = df['issue_d'].dt.month\n",
        "\n",
        "# Categorical flags\n",
        "df['recent_payment'] = (df['time_since_last_payment'] <= 30).astype(int)\n",
        "df['recent_credit_pull'] = (df['time_since_last_credit_pull'] <= 90).astype(int)\n",
        "\n",
        "drop = {'issue_d','earliest_cr_line','last_pymnt_d','last_credit_pull_d'}\n",
        "df.drop(drop,axis=1, inplace=True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-27T09:17:30.386053Z",
          "iopub.execute_input": "2025-02-27T09:17:30.386303Z",
          "iopub.status.idle": "2025-02-27T09:17:31.344526Z",
          "shell.execute_reply.started": "2025-02-27T09:17:30.386282Z",
          "shell.execute_reply": "2025-02-27T09:17:31.342437Z"
        },
        "id": "ZGcmkAzI3tzq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Numeric Features"
      ],
      "metadata": {
        "id": "DkZ0INMk3tzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = df.loan_status_binary\n",
        "cat_cols = df.select_dtypes(include=['object']).columns\n",
        "num_cols = df.select_dtypes(exclude=['object']).drop('loan_status_binary',axis=1).columns\n",
        "\n",
        "# Concatenate numeric columns explicitly\n",
        "df_final = pd.concat([df[num_cols], pd.get_dummies(df[cat_cols], drop_first=True)], axis=1)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-27T09:17:31.345481Z",
          "iopub.execute_input": "2025-02-27T09:17:31.345782Z",
          "iopub.status.idle": "2025-02-27T09:17:36.264153Z",
          "shell.execute_reply.started": "2025-02-27T09:17:31.345752Z",
          "shell.execute_reply": "2025-02-27T09:17:36.262709Z"
        },
        "id": "kcsKaklH3tzq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "external_scores = {'last_fico_range_high','last_fico_range_low','fico_range_low', 'fico_range_high'}\n",
        "df_final.drop(external_scores , axis=1, inplace=True)\n",
        "df_final.drop('id', axis=1, inplace=True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-27T09:17:36.265312Z",
          "iopub.execute_input": "2025-02-27T09:17:36.265599Z",
          "iopub.status.idle": "2025-02-27T09:17:37.27854Z",
          "shell.execute_reply.started": "2025-02-27T09:17:36.26557Z",
          "shell.execute_reply": "2025-02-27T09:17:37.277231Z"
        },
        "id": "Z_mgQrqX3tzq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Define imputer (mean for numeric features, most frequent for categorical)\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "\n",
        "# Apply imputation to X\n",
        "X_imputed = imputer.fit_transform(df_final)\n",
        "\n",
        "# Convert back to DataFrame if necessary\n",
        "df_final = pd.DataFrame(X_imputed, columns=df_final.columns)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-27T09:17:37.279549Z",
          "iopub.execute_input": "2025-02-27T09:17:37.280058Z",
          "iopub.status.idle": "2025-02-27T09:18:06.605397Z",
          "shell.execute_reply.started": "2025-02-27T09:17:37.280029Z",
          "shell.execute_reply": "2025-02-27T09:18:06.604178Z"
        },
        "id": "QktpAsLS3tzq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Correlation Analysis\n",
        "Identify features strongly correlated with the target variable."
      ],
      "metadata": {
        "id": "H0O3o6303tzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the absolute correlations\n",
        "correlations = df_final.corrwith(y).abs()\n",
        "\n",
        "# Convert to DataFrame and sort\n",
        "correlation_df = correlations.sort_values(ascending=False).reset_index()\n",
        "correlation_df.columns = ['name', 'correlation']\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "correlation_df.head(30)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-27T09:18:06.606283Z",
          "iopub.execute_input": "2025-02-27T09:18:06.606948Z",
          "iopub.status.idle": "2025-02-27T09:18:09.57916Z",
          "shell.execute_reply.started": "2025-02-27T09:18:06.606919Z",
          "shell.execute_reply": "2025-02-27T09:18:09.5777Z"
        },
        "id": "GtvqPh583tzq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute correlation matrix for independent variables\n",
        "feature_corr = df_final.corr()\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(feature_corr, annot=False, cmap='coolwarm', vmin=-1, vmax=1)\n",
        "plt.title(\"Feature Correlation Heatmap\")\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-02-27T09:49:53.212Z"
        },
        "id": "f5IjBABd3tzq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify pairs of highly correlated features\n",
        "threshold = 0.8\n",
        "high_corr_pairs = feature_corr.abs().stack().reset_index()\n",
        "high_corr_pairs = high_corr_pairs[high_corr_pairs['level_0'] != high_corr_pairs['level_1']]\n",
        "high_corr_pairs = high_corr_pairs[high_corr_pairs[0] > threshold]\n",
        "high_corr_pairs.columns = ['Feature1', 'Feature2', 'Correlation']\n",
        "high_corr_pairs"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-27T09:18:59.773669Z",
          "iopub.execute_input": "2025-02-27T09:18:59.773941Z",
          "iopub.status.idle": "2025-02-27T09:18:59.800427Z",
          "shell.execute_reply.started": "2025-02-27T09:18:59.773913Z",
          "shell.execute_reply": "2025-02-27T09:18:59.799343Z"
        },
        "id": "sxOTm7GY3tzq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìäFeature selection & training"
      ],
      "metadata": {
        "id": "EvrE3FHu3tzq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Highly correlated features can lead to redundancy, multicollinearity, and reduced model interpretability.\n",
        "> We already checked correlation with target:  Pearson correlation with the target variable to rank features,correlation matrix heatmap.\n",
        "> There is a way to drop one feature from each highly correlated pair (e.g., |correlation| > 0.8) using domain knowledge.\n",
        "> There is also an option to use Tree-Based Models for feature selection and rank features based on importance scores."
      ],
      "metadata": {
        "id": "RNPZOat53tzr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Selection based on correlation"
      ],
      "metadata": {
        "id": "yw4Sc1KN3tzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a set to store features to drop\n",
        "features_to_drop = set()\n",
        "\n",
        "# Iterate through each pair of highly correlated features\n",
        "for _, row in high_corr_pairs.iterrows():\n",
        "    feature1 = row['Feature1']\n",
        "    feature2 = row['Feature2']\n",
        "\n",
        "    # Get the correlation with the target for both features\n",
        "    corr1 = correlation_df[correlation_df['name'] == feature1]['correlation'].values[0]\n",
        "    corr2 = correlation_df[correlation_df['name'] == feature2]['correlation'].values[0]\n",
        "\n",
        "    # Drop the feature with lower correlation with the target\n",
        "    if corr1 >= corr2:\n",
        "        features_to_drop.add(feature2)\n",
        "    else:\n",
        "        features_to_drop.add(feature1)\n",
        "\n",
        "# Drop features from your dataset\n",
        "selected_features = [col for col in feature_corr.columns if col not in features_to_drop]\n",
        "\n",
        "# Print the results\n",
        "print(f\"Features to drop: {features_to_drop}\")\n",
        "print(f\"Remaining features: {len(selected_features)}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-27T09:18:59.801249Z",
          "iopub.execute_input": "2025-02-27T09:18:59.801498Z",
          "iopub.status.idle": "2025-02-27T09:18:59.870736Z",
          "shell.execute_reply.started": "2025-02-27T09:18:59.801476Z",
          "shell.execute_reply": "2025-02-27T09:18:59.869879Z"
        },
        "id": "APuFZK2b3tzr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "> From highly correlated features  loan-amount and installment lets do a one new feature"
      ],
      "metadata": {
        "id": "mM4K7_he3tzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_final['loan_amnt_div_instlmnt']=df_final['loan_amnt']/df_final['installment']"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-27T09:18:59.871777Z",
          "iopub.execute_input": "2025-02-27T09:18:59.872055Z",
          "iopub.status.idle": "2025-02-27T09:18:59.888893Z",
          "shell.execute_reply.started": "2025-02-27T09:18:59.87203Z",
          "shell.execute_reply": "2025-02-27T09:18:59.888092Z"
        },
        "id": "tAfXai0s3tzr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "drop={'funded_amnt_inv', 'total_bal_il', 'total_pymnt', 'num_rev_tl_bal_gt_0',\n",
        "      'issue_year', 'num_sats', 'num_op_rev_tl', 'total_bc_limit',\n",
        "      'total_il_high_credit_limit', 'credit_history_length', 'num_bc_tl',\n",
        "      'out_prncp_inv', 'num_tl_30dpd', 'bc_util', 'tot_cur_bal',\n",
        "      'num_actv_bc_tl', 'open_acc', 'funded_amnt', 'loan_amnt',\n",
        "      'installment','collection_recovery_fee', 'total_pymnt_inv',\n",
        "      'revol_bal', 'revol_util%', 'num_bc_sats'}\n",
        "df_final.drop(drop,axis=1, inplace=True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-27T09:18:59.890135Z",
          "iopub.execute_input": "2025-02-27T09:18:59.890359Z",
          "iopub.status.idle": "2025-02-27T09:19:00.335223Z",
          "shell.execute_reply.started": "2025-02-27T09:18:59.890337Z",
          "shell.execute_reply": "2025-02-27T09:19:00.334054Z"
        },
        "id": "gmr3FzgF3tzr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Further features selection with RFE"
      ],
      "metadata": {
        "id": "OXobiNdP3tzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_final"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-27T09:19:00.336259Z",
          "iopub.execute_input": "2025-02-27T09:19:00.336534Z",
          "iopub.status.idle": "2025-02-27T09:19:00.340025Z",
          "shell.execute_reply.started": "2025-02-27T09:19:00.336507Z",
          "shell.execute_reply": "2025-02-27T09:19:00.339249Z"
        },
        "id": "4zHsCcSA3tzr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-27T09:19:00.341109Z",
          "iopub.execute_input": "2025-02-27T09:19:00.341341Z",
          "iopub.status.idle": "2025-02-27T09:19:04.444409Z",
          "shell.execute_reply.started": "2025-02-27T09:19:00.341318Z",
          "shell.execute_reply": "2025-02-27T09:19:04.443044Z"
        },
        "id": "z1JuVloF3tzr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Random Forest for feature selection\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Select features with importance above a threshold\n",
        "model = SelectFromModel(rf, prefit=True, threshold=\"mean\")\n",
        "X_train_selected = model.transform(X_train_scaled)\n",
        "X_test_selected = model.transform(X_test_scaled)\n",
        "\n",
        "print(f\"Original features: {X.shape[1]}, Selected features: {X_train_selected.shape[1]}\")\n",
        "\n",
        "# Get the mask of selected features\n",
        "selected_features_mask = model.get_support()\n",
        "\n",
        "# Get the names of the selected features\n",
        "selected_features_names = np.array(X.columns)[selected_features_mask]\n",
        "selected_features_names"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-27T09:19:04.445541Z",
          "iopub.execute_input": "2025-02-27T09:19:04.446071Z",
          "iopub.status.idle": "2025-02-27T09:33:50.934345Z",
          "shell.execute_reply.started": "2025-02-27T09:19:04.446042Z",
          "shell.execute_reply": "2025-02-27T09:33:50.932974Z"
        },
        "id": "v6QZ2lND3tzs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "RESULT: 'out_prncp', 'total_rec_prncp', 'total_rec_int',\n",
        "       'total_rec_late_fee', 'recoveries', 'last_pymnt_amnt',\n",
        "       'debt_settlement_flag', 'int_rate%', 'loan_age',\n",
        "       'time_since_last_payment', 'time_since_last_credit_pull',\n",
        "       'loan_amnt_div_instlmnt'"
      ],
      "metadata": {
        "id": "Uctvyml53tzs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üí†Training Models"
      ],
      "metadata": {
        "id": "hcdytSXd3tzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to evaluate and display metrics\n",
        "def evaluate_model(model, X_test, y_test, model_name):\n",
        "    y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f2 = fbeta_score(y_test, y_pred, beta=2)\n",
        "    auc = roc_auc_score(y_test, y_pred_prob)\n",
        "\n",
        "    print(f\"Results for {model_name}:\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(conf_matrix)\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F2 Score: {f2:.4f}\")\n",
        "    print(f\"AUC: {auc:.4f}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "    return {\n",
        "        \"Model\": model_name,\n",
        "        \"Accuracy\": accuracy,\n",
        "        \"Recall\": recall,\n",
        "        \"F2\": f2,\n",
        "        \"AUC\": auc\n",
        "    }\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-27T09:33:50.935173Z",
          "iopub.execute_input": "2025-02-27T09:33:50.935463Z",
          "iopub.status.idle": "2025-02-27T09:33:50.941431Z",
          "shell.execute_reply.started": "2025-02-27T09:33:50.935436Z",
          "shell.execute_reply": "2025-02-27T09:33:50.940482Z"
        },
        "id": "Iz9AAszz3tzs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  1. Logistic Regression\n",
        "##  2. Random Forest\n",
        "##  3. Gradient Boost\n",
        "##  4. Neural Networks"
      ],
      "metadata": {
        "id": "RfVMcZ5I3tzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, accuracy_score, recall_score, fbeta_score, roc_auc_score,\n",
        "    classification_report, RocCurveDisplay)\n",
        "\n",
        "# Logistic Regression Model\n",
        "lr_model = LogisticRegression(random_state=42)\n",
        "lr_model.fit(X_train_selected, y_train)\n",
        "\n",
        "# Random Forest Model\n",
        "rf_soft = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_soft.fit(X_train_selected, y_train)\n",
        "\n",
        "# Gradient Boosting Model\n",
        "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "gb_model.fit(X_train_selected, y_train)\n",
        "\n",
        "# Neural Network Model\n",
        "nn_model = MLPClassifier(hidden_layer_sizes=(50, 25), max_iter=300, random_state=42, solver='adam')\n",
        "nn_model.fit(X_train_selected, y_train)\n",
        "\n",
        "# Update the evaluation code to include all four models\n",
        "results = []\n",
        "results.append(evaluate_model(lr_model, X_test_selected, y_test, \"Logistic Regression\"))\n",
        "results.append(evaluate_model(rf_soft, X_test_selected, y_test, \"Random Forest\"))\n",
        "results.append(evaluate_model(gb_model, X_test_selected, y_test, \"Gradient Boosting\"))\n",
        "results.append(evaluate_model(nn_model, X_test_selected, y_test, \"Neural Network\"))\n",
        "\n",
        "# Plot ROC Curves for all models\n",
        "plt.figure(figsize=(4, 4))\n",
        "RocCurveDisplay.from_estimator(lr_model, X_test_selected, y_test, name=\"Logistic Regression\")\n",
        "RocCurveDisplay.from_estimator(rf_soft, X_test_selected, y_test, name=\"Random Forest\")\n",
        "RocCurveDisplay.from_estimator(gb_model, X_test_selected, y_test, name=\"Gradient Boosting\")\n",
        "RocCurveDisplay.from_estimator(nn_model, X_test_selected, y_test, name=\"Neural Network\")\n",
        "plt.title(\"ROC Curve Comparison\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# Summarize the results in a DataFrame\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"Summary of Results:\")\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-27T09:33:50.942439Z",
          "iopub.execute_input": "2025-02-27T09:33:50.942711Z",
          "execution_failed": "2025-02-27T09:49:53.212Z"
        },
        "id": "yGNtgBHI3tzs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìúConclusion\n",
        "> The Neural Network and Random Forest model has quite good scores! We could check a different thresholds to see if the FP and FN can have a better results.\n",
        "> <br>\n",
        "> In next notebook we will create a behavioral scorecard: the general framework involves setting score ranges and corresponding risk groups (e.g., high-risk, medium-risk, low-risk) based on statistical analysis and business policies.<br>\n",
        "> To not interupt the code and make the steps clear I'm saving the important results:<br>\n",
        "> * Neural Network  model<br>\n",
        "> * Random Forest model<br>\n",
        "> * data after cleaning with selected features.<br>\n",
        "Everything here:<br>\n",
        "https://www.kaggle.com/datasets/beatafaron/loan-credit-risk-and-population-stability\n"
      ],
      "metadata": {
        "id": "wUtgVhgJ3tzt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üéØNext Steps\n",
        "> Here is direct link to next steps: <br>\n",
        "> 2. Behavioral Scorecards, weight of evidence, logistic regresion.\n",
        "> https://www.kaggle.com/code/beatafaron/complete-credit-risk-modeling-2-sc-woe\n",
        "> <br>\n",
        "> 3. Population stability index.<br>\n",
        "> https://www.kaggle.com/code/beatafaron/complete-credit-risk-modeling-3-psi"
      ],
      "metadata": {
        "id": "4KfeuXOs3tzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save models for later"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-02-27T09:49:53.212Z"
        },
        "id": "aNtyIJgS3tzt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "# Save the NN model to a file\n",
        "joblib.dump(nn_model, 'nn_model_model.pkl')\n",
        "\n",
        "# Save the RF model to a file\n",
        "joblib.dump(rf_soft, 'rf_soft_model.pkl')\n",
        "\n",
        "# Save df_final to a CSV file\n",
        "df_final = df_final[selected_features_names]\n",
        "df_final.to_csv(\"df_2014-18_selected.csv\", index=False)"
      ],
      "metadata": {
        "execution": {
          "execution_failed": "2025-02-27T09:49:53.212Z"
        },
        "trusted": true,
        "id": "E16ntMbB3tzt"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}