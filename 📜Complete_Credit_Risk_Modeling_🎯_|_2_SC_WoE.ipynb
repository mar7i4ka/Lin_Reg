{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 44744,
          "sourceType": "datasetVersion",
          "datasetId": 33672
        },
        {
          "sourceId": 370089,
          "sourceType": "datasetVersion",
          "datasetId": 902
        },
        {
          "sourceId": 1747344,
          "sourceType": "datasetVersion",
          "datasetId": 772636
        },
        {
          "sourceId": 2344503,
          "sourceType": "datasetVersion",
          "datasetId": 1415343
        },
        {
          "sourceId": 10611214,
          "sourceType": "datasetVersion",
          "datasetId": 6420200
        }
      ],
      "dockerImageVersionId": 30839,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "📜Complete Credit Risk Modeling 🎯 | 2. SC  WoE  ",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mar7i4ka/Lin_Reg/blob/main/%F0%9F%93%9CComplete_Credit_Risk_Modeling_%F0%9F%8E%AF_%7C_2_SC_WoE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "husainsb_lendingclub_issued_loans_path = kagglehub.dataset_download('husainsb/lendingclub-issued-loans')\n",
        "wordsforthewise_lending_club_path = kagglehub.dataset_download('wordsforthewise/lending-club')\n",
        "ethon0426_lending_club_20072020q1_path = kagglehub.dataset_download('ethon0426/lending-club-20072020q1')\n",
        "adarshsng_lending_club_loan_data_csv_path = kagglehub.dataset_download('adarshsng/lending-club-loan-data-csv')\n",
        "beatafaron_loan_credit_risk_and_population_stability_path = kagglehub.dataset_download('beatafaron/loan-credit-risk-and-population-stability')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "E7j1LLYp3lAp"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This is a continuation\n",
        "> of a notebook:\n",
        "> 📜Complete Credit Risk Modeling 🔍 | 1. EDA\n",
        "> direct link here:\n",
        "https://www.kaggle.com/code/beatafaron/complete-credit-risk-modeling-1-eda"
      ],
      "metadata": {
        "id": "crfLgntI3lAq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.📘 SCOPE\n",
        "\n",
        "> In contiunation we will create a behavioral scorecard: the general framework involves setting score ranges and corresponding risk groups (e.g., high-risk, medium-risk, low-risk) based on statistical analysis and business policies.\n",
        ">\n",
        "\n",
        "\n",
        "## **1. 1. Behavioral Scorecards**\n",
        "\n",
        "**Score Distribution**:\n",
        "   - Behavioral scorecards typically assign scores on a scale (e.g., 300–900) that reflects the likelihood of favorable behavior (e.g., repayment).\n",
        "   - Higher scores indicate lower risk (e.g., low probability of default).\n",
        "   - Lower scores indicate higher risk (e.g., high probability of default).\n",
        "   - Risk thresholds are typically based on statistical analysis (e.g., percentiles or clustering) or business rules (e.g., cutoffs for default rates).\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## **1. 2. Rules for Assigning Scores**\n",
        "\n",
        "1. **Weight of Evidence (WOE) Transformation**:\n",
        "   - Continuous features (e.g., credit utilization, payment-to-balance ratio) and categorical features are binned and transformed into WOE values for score calculation.\n",
        "\n",
        "2. **Logistic Regression**:\n",
        "   - Scores are typically derived from logistic regression models, where the predicted probability of default (PD) is converted into a score.\n",
        "\n",
        "3. **Score Scaling**:\n",
        "   - Scores are scaled to a user-friendly range using the formula:\n",
        "$$\n",
        "\\[\n",
        "\\text{Score} = \\text{Base Score} + \\text{Factor} \\times \\log \\left( \\frac{1 - \\text{PD}}{\\text{PD}} \\right)\n",
        "\\]    \n",
        "$$     \n",
        "     Where:\n",
        "     - **Base Score**: Starting point for scores (e.g., 300).\n",
        "     - **Factor**: Controls the distribution (e.g., 20 per doubling of odds).\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "_sQrJslV3lAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Jupyter: scikit-learn 1.5.1\n",
        "Kaggle: scikit-learn 1.2.2 (Older version!)\n",
        "This version mismatch is causing the huge difference in logistic regression coefficients (intercept_ values).\n",
        "\n",
        "model.intercept_ in kaggle : array([-7.52160171])\n",
        "model.intercept_ in jupiter : array([-3.48804904])\n",
        "\"\"\"\n",
        "!pip install --upgrade scikit-learn==1.5.1\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:29:23.590093Z",
          "iopub.execute_input": "2025-02-26T20:29:23.590575Z",
          "iopub.status.idle": "2025-02-26T20:29:28.519681Z",
          "shell.execute_reply.started": "2025-02-26T20:29:23.590537Z",
          "shell.execute_reply": "2025-02-26T20:29:28.518337Z"
        },
        "id": "g6IyCzYD3lAs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"scikit-learn version:\", sklearn.__version__)\n",
        "print(\"pandas version:\", pd.__version__)\n",
        "print(\"numpy version:\", np.__version__)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:29:28.521274Z",
          "iopub.execute_input": "2025-02-26T20:29:28.521625Z",
          "iopub.status.idle": "2025-02-26T20:29:29.451811Z",
          "shell.execute_reply.started": "2025-02-26T20:29:28.521593Z",
          "shell.execute_reply": "2025-02-26T20:29:29.450734Z"
        },
        "id": "OVMZJ-K73lAs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.options.display.max_columns = None"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:29:29.454059Z",
          "iopub.execute_input": "2025-02-26T20:29:29.454653Z",
          "iopub.status.idle": "2025-02-26T20:29:29.460119Z",
          "shell.execute_reply.started": "2025-02-26T20:29:29.454617Z",
          "shell.execute_reply": "2025-02-26T20:29:29.458774Z"
        },
        "id": "nTCdeauf3lAt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.📌Read & Bin"
      ],
      "metadata": {
        "id": "4EQmxWT33lAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/kaggle/input/loan-credit-risk-and-population-stability/df_2014-18_selected.csv')\n",
        "dictionary=pd.read_csv('/kaggle/input/loan-credit-risk-and-population-stability/dictionary_selected.csv')\n",
        "df = df.round(3)\n",
        "df.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:29:29.461867Z",
          "iopub.execute_input": "2025-02-26T20:29:29.462209Z",
          "iopub.status.idle": "2025-02-26T20:29:33.523626Z",
          "shell.execute_reply.started": "2025-02-26T20:29:29.462179Z",
          "shell.execute_reply": "2025-02-26T20:29:33.522449Z"
        },
        "id": "87kuqrNS3lAt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📌 Binning\n",
        "\n",
        "Here  I am including a small digression on quick and clever data binning. We will build two functions to choose the best way for smart binning the contiunos variables.\n",
        "\n",
        "1. First one **`bin_and_plot_woe_manual`** → Uses `pd.cut()` to bin a continuous variable and plots WOE.  \n",
        "2. Second one **`bin_and_plot_woe_tree`** → Uses `DecisionTreeClassifier` to bin a continuous variable and plots WOE.  \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "###  **Differences Between the Two Methods**\n",
        "| Method | Description | Pros | Cons |\n",
        "|--------|------------|------|------|\n",
        "| **`pd.cut()` (Equal Width Binning)** | Divides data into `bins` of equal width | Simple, interpretable | Might not capture patterns well |\n",
        "| **DecisionTreeClassifier Binning** | Uses tree-based splits to define bins | Data-driven, captures patterns | Can overfit if `max_depth` is too high |\n",
        "\n",
        "---\n",
        "Let's check it out!\n"
      ],
      "metadata": {
        "id": "9qCeSIbI3lAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.total_rec_late_fee.describe()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:29:33.524771Z",
          "iopub.execute_input": "2025-02-26T20:29:33.52529Z",
          "iopub.status.idle": "2025-02-26T20:29:33.612585Z",
          "shell.execute_reply.started": "2025-02-26T20:29:33.525245Z",
          "shell.execute_reply": "2025-02-26T20:29:33.611331Z"
        },
        "id": "ybxHhHlp3lAu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Although the mean and the standard deviation is around 2, the max value is very huge - 1.598520e+03. Propably we have outliers. Lets check how it influence the binning."
      ],
      "metadata": {
        "id": "XvTQtNJc3lAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_woe_distribution(woe_df, feature):\n",
        "    \"\"\"\n",
        "    Plot WOE distribution.\n",
        "\n",
        "    Args:\n",
        "        woe_df (pd.DataFrame): DataFrame with bins and WOE values.\n",
        "        feature (str): Feature name for title.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(4, 3))\n",
        "    bin_labels = [str(b) for b in woe_df['bin']]\n",
        "    plt.plot(bin_labels, woe_df['WOE'], marker='o', linestyle='-', color='lightblue', label='WOE')\n",
        "\n",
        "    # Style improvements\n",
        "    plt.xlabel('Bins', fontsize=8)\n",
        "    plt.ylabel('Weight of Evidence (WOE)', fontsize=8)\n",
        "    plt.title(f'WOE Distribution for {feature}', fontsize=8)\n",
        "    plt.xticks(rotation=45, ha='right', fontsize=8)\n",
        "    plt.yticks(fontsize=8)\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.legend(fontsize=10)\n",
        "\n",
        "    # Remove top and right spines (black borders)\n",
        "    ax = plt.gca()\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    ax.spines['left'].set_visible(False)\n",
        "    ax.spines['bottom'].set_visible(False)\n",
        "    plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:29:33.61381Z",
          "iopub.execute_input": "2025-02-26T20:29:33.61423Z",
          "iopub.status.idle": "2025-02-26T20:29:33.845807Z",
          "shell.execute_reply.started": "2025-02-26T20:29:33.614188Z",
          "shell.execute_reply": "2025-02-26T20:29:33.844638Z"
        },
        "id": "YFpBcJqh3lAu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.🎯WoE 🎯IV"
      ],
      "metadata": {
        "id": "fI_lfdha3lAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bin_and_plot_woe_manual(data, feature, target, bins=5, eps=0.0001):\n",
        "    \"\"\"\n",
        "    Bin a continuous variable using equal-width binning (pd.cut) and plot WOE.\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame): The dataset containing the feature and target.\n",
        "        feature (str): The name of the continuous feature.\n",
        "        target (str): The binary target variable.\n",
        "        bins (int): Number of bins for pd.cut().\n",
        "        eps (float): Small value to prevent division by zero.\n",
        "\n",
        "    Returns:\n",
        "        woe_df (pd.DataFrame): DataFrame with bins and WOE values.\n",
        "    \"\"\"\n",
        "    # Create bins using pd.cut\n",
        "    data['bin'] = pd.cut(data[feature], bins=bins, include_lowest=True, duplicates='drop')\n",
        "\n",
        "    # Aggregate event & non-event counts\n",
        "    woe_df = data.groupby('bin', observed=True).agg(\n",
        "        total_count=(target, 'count'),\n",
        "        event_count=(target, 'sum')\n",
        "    ).reset_index()\n",
        "\n",
        "    woe_df['non_event_count'] = woe_df['total_count'] - woe_df['event_count']\n",
        "\n",
        "    # Compute event and non-event rates (avoid division by zero)\n",
        "    woe_df['event_rate'] = (woe_df['event_count'] + eps) / woe_df['event_count'].sum()\n",
        "    woe_df['non_event_rate'] = (woe_df['non_event_count'] + eps) / woe_df['non_event_count'].sum()\n",
        "\n",
        "    # Compute WOE\n",
        "    woe_df['WOE'] = round(np.log(woe_df['non_event_rate'] / woe_df['event_rate']),4)\n",
        "\n",
        "     # Compute IV for each bin\n",
        "    woe_df['IV'] = round((woe_df['non_event_rate'] - woe_df['event_rate']) * woe_df['WOE'],4)\n",
        "\n",
        "    # Compute total IV\n",
        "    total_IV = woe_df['IV'].sum()\n",
        "    print(f'Total IV for {feature}: {total_IV:.4f}')\n",
        "\n",
        "    # Convert bins to string format for plotting\n",
        "    # woe_df['bin_str'] = woe_df['bin'].astype(str)\n",
        "\n",
        "    # Plot WOE distribution\n",
        "    plot_woe_distribution(woe_df, feature)\n",
        "\n",
        "    return woe_df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-26T20:29:33.846908Z",
          "iopub.execute_input": "2025-02-26T20:29:33.847642Z",
          "iopub.status.idle": "2025-02-26T20:29:33.856597Z",
          "shell.execute_reply.started": "2025-02-26T20:29:33.847603Z",
          "shell.execute_reply": "2025-02-26T20:29:33.855258Z"
        },
        "trusted": true,
        "id": "08VtbfRD3lAv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from decimal import Decimal, ROUND_DOWN\n",
        "\n",
        "def truncate(value, decimals=3):\n",
        "    factor = Decimal('1.' + '0' * decimals)\n",
        "    return float(Decimal(value).quantize(factor, rounding=ROUND_DOWN))\n",
        "\n",
        "\n",
        "def bin_and_plot_woe_tree(data, feature, target, max_depth=4, min_samples_leaf=2000, eps=0.0001):\n",
        "    \"\"\"\n",
        "    Bin a continuous variable using DecisionTreeClassifier and compute WOE.\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame): The dataset containing the feature and target.\n",
        "        feature (str): The name of the continuous feature.\n",
        "        target (str): The binary target variable.\n",
        "        max_depth (int): Max depth of the decision tree for binning.\n",
        "        min_samples_leaf (int): Minimum samples per leaf to avoid overfitting.\n",
        "        eps (float): Small value to prevent division by zero.\n",
        "\n",
        "    Returns:\n",
        "        woe_df (pd.DataFrame): DataFrame with bins, WOE values, and IV.\n",
        "    \"\"\"\n",
        "    # Fit Decision Tree for binning\n",
        "    X = data[[feature]]\n",
        "    y = data[target]\n",
        "    tree = DecisionTreeClassifier(max_depth=max_depth, min_samples_leaf=min_samples_leaf, random_state=42)\n",
        "    tree.fit(X, y)\n",
        "\n",
        "    # Get bin edges from decision tree and truncate to 3 decimal places\n",
        "    thresholds = tree.tree_.threshold\n",
        "    thresholds = thresholds[thresholds != -2]  # Remove leaf node markers\n",
        "    thresholds = sorted(thresholds)\n",
        "    thresholds = [truncate(th, 3) for th in thresholds]\n",
        "\n",
        "    # Define bin edges\n",
        "    bins = [truncate(data[feature].min(), 3)] + thresholds + [truncate(data[feature].max(), 3)]\n",
        "    bins[0] = truncate(data[feature].min(), 3)\n",
        "\n",
        "    # Bin the data\n",
        "    data['bin'] = pd.cut(data[feature], bins=bins, include_lowest=True)\n",
        "\n",
        "    # Aggregate event & non-event counts\n",
        "    woe_df = data.groupby('bin').agg(\n",
        "        total_count=(target, 'count'),\n",
        "        event_count=(target, 'sum')\n",
        "    ).reset_index()\n",
        "    woe_df['non_event_count'] = woe_df['total_count'] - woe_df['event_count']\n",
        "\n",
        "    # Compute event and non-event rates (avoid division by zero)\n",
        "    woe_df['event_rate'] = (woe_df['event_count'] + eps) / woe_df['event_count'].sum()\n",
        "    woe_df['non_event_rate'] = (woe_df['non_event_count'] + eps) / woe_df['non_event_count'].sum()\n",
        "\n",
        "    # Compute WOE\n",
        "    woe_df['WOE'] = np.log(woe_df['non_event_rate'] / woe_df['event_rate'])\n",
        "\n",
        "    # Compute IV for each bin\n",
        "    woe_df['IV'] = (woe_df['non_event_rate'] - woe_df['event_rate']) * woe_df['WOE']\n",
        "\n",
        "    # Compute total IV\n",
        "    total_IV = woe_df['IV'].sum()\n",
        "    print(f'Total IV for {feature}: {total_IV:.4f}')\n",
        "\n",
        "    # Plot WOE distribution\n",
        "    plot_woe_distribution(woe_df, feature)\n",
        "\n",
        "    return woe_df\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:29:33.859995Z",
          "iopub.execute_input": "2025-02-26T20:29:33.860311Z",
          "iopub.status.idle": "2025-02-26T20:29:34.023476Z",
          "shell.execute_reply.started": "2025-02-26T20:29:33.860284Z",
          "shell.execute_reply": "2025-02-26T20:29:34.022341Z"
        },
        "id": "XKXcvwM03lAv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Weight of evidence - Equal Width Binning\")\n",
        "woe_manual = bin_and_plot_woe_manual(df, 'total_rec_late_fee', 'loan_status_binary', bins=5)\n",
        "print(\"Weight of evidence - Decision Tree Classifier Binning\")\n",
        "woe_tree = bin_and_plot_woe_tree(df, 'total_rec_late_fee', 'loan_status_binary', max_depth=4)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:29:34.026102Z",
          "iopub.execute_input": "2025-02-26T20:29:34.026665Z",
          "iopub.status.idle": "2025-02-26T20:29:34.977701Z",
          "shell.execute_reply.started": "2025-02-26T20:29:34.026633Z",
          "shell.execute_reply": "2025-02-26T20:29:34.976405Z"
        },
        "id": "VGtimKlK3lAv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Check how different results we have! Both plots have different bins - with the manual plot we should check the outliers and try binning without them.\n",
        "> Look how different Importance Values  we can achieve by properly grouping data. <br>\n",
        "> 1. Total IV for total_rec_late_fee: 0.0004\n",
        ">2. Total IV for total_rec_late_fee: 0.2404 <br>\n"
      ],
      "metadata": {
        "id": "im6QlymV3lAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "woe_tree.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:29:34.978805Z",
          "iopub.execute_input": "2025-02-26T20:29:34.979157Z",
          "iopub.status.idle": "2025-02-26T20:29:34.994152Z",
          "shell.execute_reply.started": "2025-02-26T20:29:34.979127Z",
          "shell.execute_reply": "2025-02-26T20:29:34.992919Z"
        },
        "id": "azHYfaHP3lAw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "woe_manual.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:29:34.995552Z",
          "iopub.execute_input": "2025-02-26T20:29:34.995986Z",
          "iopub.status.idle": "2025-02-26T20:29:35.024675Z",
          "shell.execute_reply.started": "2025-02-26T20:29:34.995942Z",
          "shell.execute_reply": "2025-02-26T20:29:35.023549Z"
        },
        "id": "n6atepJy3lAw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "> As we observe - there are two outliers. Also the main part of distribution is between -1,59 to 319 ( looking on manual), where DecisionTreeClassifier narrow this bin from -inf to 0.035"
      ],
      "metadata": {
        "id": "cahgUm6Q3lAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/kaggle/input/loan-credit-risk-and-population-stability/df_2014-18_selected.csv')\n",
        "df = df.round(3)\n",
        "df.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:29:35.025809Z",
          "iopub.execute_input": "2025-02-26T20:29:35.026127Z",
          "iopub.status.idle": "2025-02-26T20:29:38.332662Z",
          "shell.execute_reply.started": "2025-02-26T20:29:35.026098Z",
          "shell.execute_reply": "2025-02-26T20:29:38.331538Z"
        },
        "id": "ziNfYgko3lAw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:29:38.333703Z",
          "iopub.execute_input": "2025-02-26T20:29:38.333969Z",
          "iopub.status.idle": "2025-02-26T20:29:38.353805Z",
          "shell.execute_reply.started": "2025-02-26T20:29:38.333947Z",
          "shell.execute_reply": "2025-02-26T20:29:38.352547Z"
        },
        "id": "H2MjGuhW3lAw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# set target and features\n",
        "target = 'loan_status_binary'\n",
        "features = df.columns.drop(target).tolist()\n",
        "features"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:29:38.354952Z",
          "iopub.execute_input": "2025-02-26T20:29:38.355305Z",
          "iopub.status.idle": "2025-02-26T20:29:38.381739Z",
          "shell.execute_reply.started": "2025-02-26T20:29:38.355272Z",
          "shell.execute_reply": "2025-02-26T20:29:38.38038Z"
        },
        "id": "GTjRCmqV3lAw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_prep=pd.DataFrame()\n",
        "dropped_first=pd.DataFrame()\n",
        "\n",
        "def transform_to_dummy(data, feature, woe_df, df_prep=None, dropped_first=None):\n",
        "    \"\"\"\n",
        "    Transform a continuous feature into categorical dummy variables based on binning.\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame): The original dataset.\n",
        "        feature (str): The name of the continuous feature to transform.\n",
        "        woe_df (pd.DataFrame): DataFrame containing bin information from binning.\n",
        "        df_prep (pd.DataFrame, optional): DataFrame to store transformed categorical dummy variables.\n",
        "        dropped_first (pd.DataFrame, optional): DataFrame to store dropped bin information.\n",
        "\n",
        "    Returns:\n",
        "        df_prep (pd.DataFrame): Updated dataset with only the transformed categorical dummy variables.\n",
        "        dropped_first (pd.DataFrame): Updated DataFrame storing the feature and dropped bin information.\n",
        "    \"\"\"\n",
        "    # Ensure bins are applied to the original data\n",
        "    bin_labels = woe_df['bin'].astype(str).values\n",
        "    data['bin'] = pd.cut(\n",
        "        data[feature],\n",
        "        bins=[b.left for b in woe_df['bin'].cat.categories] + [woe_df['bin'].cat.categories[-1].right],\n",
        "        include_lowest=True,\n",
        "        labels=bin_labels\n",
        "    )\n",
        "\n",
        "    # Identify the dropped bin\n",
        "    dropped_bin = bin_labels[0] if len(bin_labels) > 0 else None\n",
        "    new_dropped = pd.DataFrame({'feature': [feature], 'dropped_bin': [dropped_bin], 'col_name': [feature + \"_\" + dropped_bin]})\n",
        "\n",
        "    # Convert categorical bins into dummy variables with proper naming\n",
        "    new_df_prep = pd.get_dummies(data[['bin']], columns=['bin'], prefix=f\"{feature}\", drop_first=False)\n",
        "\n",
        "    # Append to existing dataframes if provided\n",
        "    if df_prep is not None:\n",
        "        df_prep = pd.concat([df_prep, new_df_prep], axis=1)\n",
        "    else:\n",
        "        df_prep = new_df_prep\n",
        "\n",
        "    if dropped_first is not None:\n",
        "        dropped_first = pd.concat([dropped_first, new_dropped], ignore_index=True)\n",
        "    else:\n",
        "        dropped_first = new_dropped\n",
        "\n",
        "    return df_prep, dropped_first\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:29:38.382914Z",
          "iopub.execute_input": "2025-02-26T20:29:38.383296Z",
          "iopub.status.idle": "2025-02-26T20:29:38.408098Z",
          "shell.execute_reply.started": "2025-02-26T20:29:38.383262Z",
          "shell.execute_reply": "2025-02-26T20:29:38.406742Z"
        },
        "id": "dCIICrzA3lAw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 1. Using Decision Tree Classifier"
      ],
      "metadata": {
        "id": "3wT_Caa_3lAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for feature in features:\n",
        "    woe_tree = bin_and_plot_woe_tree(df, feature, 'loan_status_binary', max_depth=4)\n",
        "    df_prep, dropped_first = transform_to_dummy(df, feature, woe_tree, df_prep, dropped_first)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:29:38.40953Z",
          "iopub.execute_input": "2025-02-26T20:29:38.410004Z",
          "iopub.status.idle": "2025-02-26T20:30:01.439815Z",
          "shell.execute_reply.started": "2025-02-26T20:29:38.409961Z",
          "shell.execute_reply": "2025-02-26T20:30:01.438563Z"
        },
        "id": "VDv9XRD83lAx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Great! Now lets check how look our data after binning and transforming to dummy variables.\n",
        ">\n",
        "> First columns to drop are stored in dropped_first"
      ],
      "metadata": {
        "id": "Uf-NH6lZ3lAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_prep.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:30:01.440687Z",
          "iopub.execute_input": "2025-02-26T20:30:01.441118Z",
          "iopub.status.idle": "2025-02-26T20:30:01.534501Z",
          "shell.execute_reply.started": "2025-02-26T20:30:01.441074Z",
          "shell.execute_reply": "2025-02-26T20:30:01.532846Z"
        },
        "id": "DHRLJUJf3lAx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dropped_first"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:30:01.535614Z",
          "iopub.execute_input": "2025-02-26T20:30:01.535937Z",
          "iopub.status.idle": "2025-02-26T20:30:01.548761Z",
          "shell.execute_reply.started": "2025-02-26T20:30:01.535908Z",
          "shell.execute_reply": "2025-02-26T20:30:01.547484Z"
        },
        "id": "Rmt4msbl3lAx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_prep_dropped_first=pd.DataFrame()\n",
        "df_prep_dropped_first = df_prep.drop(columns=dropped_first[\"col_name\"].tolist(), errors='ignore')\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:30:01.54994Z",
          "iopub.execute_input": "2025-02-26T20:30:01.550461Z",
          "iopub.status.idle": "2025-02-26T20:30:01.781973Z",
          "shell.execute_reply.started": "2025-02-26T20:30:01.550406Z",
          "shell.execute_reply": "2025-02-26T20:30:01.78038Z"
        },
        "id": "2Wlo33Q33lAx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dropped_first.shape, df_prep_dropped_first.shape, df_prep.shape\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:30:01.783112Z",
          "iopub.execute_input": "2025-02-26T20:30:01.783492Z",
          "iopub.status.idle": "2025-02-26T20:30:01.791595Z",
          "shell.execute_reply.started": "2025-02-26T20:30:01.783444Z",
          "shell.execute_reply": "2025-02-26T20:30:01.789949Z"
        },
        "id": "AuUfHtiH3lAx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.🤖Training Logistic Regression"
      ],
      "metadata": {
        "id": "co4UMRmy3lAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target= df['loan_status_binary']"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:30:01.79272Z",
          "iopub.execute_input": "2025-02-26T20:30:01.793062Z",
          "iopub.status.idle": "2025-02-26T20:30:01.816911Z",
          "shell.execute_reply.started": "2025-02-26T20:30:01.793032Z",
          "shell.execute_reply": "2025-02-26T20:30:01.815414Z"
        },
        "id": "P9R8t0tn3lAx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assuming df_prep contains features and target\n",
        "X = df_prep_dropped_first.copy()\n",
        "y = target  # Target variable (0/1)\n",
        "\n",
        "# Train logistic regression\n",
        "model = LogisticRegression()\n",
        "model.fit(X, y)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:30:01.818026Z",
          "iopub.execute_input": "2025-02-26T20:30:01.818632Z",
          "iopub.status.idle": "2025-02-26T20:30:17.343779Z",
          "shell.execute_reply.started": "2025-02-26T20:30:01.818579Z",
          "shell.execute_reply": "2025-02-26T20:30:17.342485Z"
        },
        "id": "WAVYKWAW3lAy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model.intercept_"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:30:17.348524Z",
          "iopub.execute_input": "2025-02-26T20:30:17.348915Z",
          "iopub.status.idle": "2025-02-26T20:30:17.355968Z",
          "shell.execute_reply.started": "2025-02-26T20:30:17.348881Z",
          "shell.execute_reply": "2025-02-26T20:30:17.354768Z"
        },
        "id": "8KUBptJ33lAy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model.coef_"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:30:17.357617Z",
          "iopub.execute_input": "2025-02-26T20:30:17.358017Z",
          "iopub.status.idle": "2025-02-26T20:30:17.382266Z",
          "shell.execute_reply.started": "2025-02-26T20:30:17.357969Z",
          "shell.execute_reply": "2025-02-26T20:30:17.380763Z"
        },
        "id": "pYsaOm7I3lAy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Now we create summary_table to store features, their coefficiences ( with intercept )"
      ],
      "metadata": {
        "id": "73Lg9MMS3lAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary_table = pd.DataFrame()\n",
        "summary_table['feature_name'] = df_prep_dropped_first.columns\n",
        "summary_table['coefficience'] = np.transpose(model.coef_)\n",
        "summary_table.loc[-1] = ['intercept', model.intercept_[0]]\n",
        "summary_table = summary_table.sort_index().reset_index(drop=True)\n",
        "summary_table"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:30:17.383656Z",
          "iopub.execute_input": "2025-02-26T20:30:17.384158Z",
          "iopub.status.idle": "2025-02-26T20:30:17.413583Z",
          "shell.execute_reply.started": "2025-02-26T20:30:17.384045Z",
          "shell.execute_reply": "2025-02-26T20:30:17.412085Z"
        },
        "id": "QyOSkST03lAy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.📈Scorecard\n",
        "> Creating scorecard scaled between 300-900"
      ],
      "metadata": {
        "id": "TEcousRV3lAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new = pd.DataFrame({'feature_name': dropped_first[\"feature\"] + \"_\" + dropped_first[\"dropped_bin\"],\n",
        "                    'coefficience': 0})\n",
        "scorecard=pd.DataFrame()\n",
        "scorecard=pd.concat([summary_table,new]).reset_index()\n",
        "scorecard"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:30:17.41478Z",
          "iopub.execute_input": "2025-02-26T20:30:17.415201Z",
          "iopub.status.idle": "2025-02-26T20:30:17.444103Z",
          "shell.execute_reply.started": "2025-02-26T20:30:17.415158Z",
          "shell.execute_reply": "2025-02-26T20:30:17.44248Z"
        },
        "id": "iqcHQuBU3lA2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "scorecard['feature_original'] = scorecard['feature_name'].str.split('_(', regex=False).str[0]\n",
        "scorecard"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:30:17.445395Z",
          "iopub.execute_input": "2025-02-26T20:30:17.445859Z",
          "iopub.status.idle": "2025-02-26T20:30:17.484197Z",
          "shell.execute_reply.started": "2025-02-26T20:30:17.445813Z",
          "shell.execute_reply": "2025-02-26T20:30:17.482921Z"
        },
        "id": "kr93xes63lA2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "scorecard.groupby('feature_original')['coefficience'].min()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:30:17.485477Z",
          "iopub.execute_input": "2025-02-26T20:30:17.485814Z",
          "iopub.status.idle": "2025-02-26T20:30:17.51263Z",
          "shell.execute_reply.started": "2025-02-26T20:30:17.485784Z",
          "shell.execute_reply": "2025-02-26T20:30:17.511518Z"
        },
        "id": "MUuMpVvc3lA3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "min_sum=scorecard.groupby('feature_original')['coefficience'].min().sum()\n",
        "max_sum=scorecard.groupby('feature_original')['coefficience'].max().sum()\n",
        "max_score=900\n",
        "min_score=300"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:30:17.51378Z",
          "iopub.execute_input": "2025-02-26T20:30:17.514162Z",
          "iopub.status.idle": "2025-02-26T20:30:17.534574Z",
          "shell.execute_reply.started": "2025-02-26T20:30:17.51413Z",
          "shell.execute_reply": "2025-02-26T20:30:17.533552Z"
        },
        "id": "7avoUSAI3lA3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "scorecard['score_cal']= scorecard['coefficience']*(max_score-min_score)/(max_sum-min_sum)\n",
        "scorecard['score_cal'][0]=(scorecard['coefficience'][0]-min_sum)/(max_sum-min_sum)*(max_score-min_score)+min_score\n",
        "scorecard"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:30:17.535766Z",
          "iopub.execute_input": "2025-02-26T20:30:17.536183Z",
          "iopub.status.idle": "2025-02-26T20:30:17.565105Z",
          "shell.execute_reply.started": "2025-02-26T20:30:17.536141Z",
          "shell.execute_reply": "2025-02-26T20:30:17.563996Z"
        },
        "id": "DVv-NEVT3lA3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "min_check=scorecard.groupby('feature_original')['score_cal'].min().sum().round()\n",
        "min_check"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:30:17.565995Z",
          "iopub.execute_input": "2025-02-26T20:30:17.566293Z",
          "iopub.status.idle": "2025-02-26T20:30:17.593206Z",
          "shell.execute_reply.started": "2025-02-26T20:30:17.566268Z",
          "shell.execute_reply": "2025-02-26T20:30:17.592002Z"
        },
        "id": "2P3UJnnc3lA3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "max_check=scorecard.groupby('feature_original')['score_cal'].max().sum().round()\n",
        "max_check"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:30:17.594248Z",
          "iopub.execute_input": "2025-02-26T20:30:17.594663Z",
          "iopub.status.idle": "2025-02-26T20:30:17.615105Z",
          "shell.execute_reply.started": "2025-02-26T20:30:17.594621Z",
          "shell.execute_reply": "2025-02-26T20:30:17.613778Z"
        },
        "id": "hcu3tkk53lA3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_prep.insert(0, 'intercept', 1)\n",
        "df_prep = df_prep[scorecard['feature_name'].values]\n",
        "scores = scorecard['score_cal']"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:30:17.61637Z",
          "iopub.execute_input": "2025-02-26T20:30:17.616842Z",
          "iopub.status.idle": "2025-02-26T20:30:17.852134Z",
          "shell.execute_reply.started": "2025-02-26T20:30:17.616787Z",
          "shell.execute_reply": "2025-02-26T20:30:17.850868Z"
        },
        "id": "XGwCWRZB3lA3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "scores.shape, df_prep.shape"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:30:17.853266Z",
          "iopub.execute_input": "2025-02-26T20:30:17.853598Z",
          "iopub.status.idle": "2025-02-26T20:30:17.860473Z",
          "shell.execute_reply.started": "2025-02-26T20:30:17.85357Z",
          "shell.execute_reply": "2025-02-26T20:30:17.859348Z"
        },
        "id": "CPEHeVkZ3lA3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "scores=scores.values.reshape(135,1)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:30:17.861363Z",
          "iopub.execute_input": "2025-02-26T20:30:17.861655Z",
          "iopub.status.idle": "2025-02-26T20:30:17.880341Z",
          "shell.execute_reply.started": "2025-02-26T20:30:17.861633Z",
          "shell.execute_reply": "2025-02-26T20:30:17.878863Z"
        },
        "id": "ShSxtn813lA3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_prep_scores = df_prep.dot(scores)\n",
        "df_prep_scores = df_prep_scores.astype(int)\n",
        "df_prep_scores"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:30:17.881444Z",
          "iopub.execute_input": "2025-02-26T20:30:17.881763Z",
          "iopub.status.idle": "2025-02-26T20:30:39.882229Z",
          "shell.execute_reply.started": "2025-02-26T20:30:17.881736Z",
          "shell.execute_reply": "2025-02-26T20:30:39.880998Z"
        },
        "id": "jTD1c7fl3lA4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.📏logistic (sigmoid) function"
      ],
      "metadata": {
        "id": "iHm2MRSR3lA4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">We have df_prep_scores, which contains the total score for each observation. Now, we need to convert these scores into probabilities using the logistic (sigmoid) function:"
      ],
      "metadata": {
        "id": "YUEX7bkM3lA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert score to probability\n",
        "y_score = 1 / (1 + np.exp(-(df_prep_scores - min_score) / (max_score - min_score)))\n",
        "\n",
        "# Flatten to a 1D array\n",
        "y_score = y_score.values.flatten()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:30:39.883102Z",
          "iopub.execute_input": "2025-02-26T20:30:39.883366Z",
          "iopub.status.idle": "2025-02-26T20:30:39.916768Z",
          "shell.execute_reply.started": "2025-02-26T20:30:39.883338Z",
          "shell.execute_reply": "2025-02-26T20:30:39.915827Z"
        },
        "id": "czrd9deP3lA4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Now, let's calculate once again the ROC-AUC score to assess model performance and visualize the effectiveness of the scorecard."
      ],
      "metadata": {
        "id": "3B7NGo9I3lA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "# Compute ROC-AUC\n",
        "auc = roc_auc_score(y, y_score)\n",
        "print(f\"ROC-AUC: {auc:.4f}\")\n",
        "\n",
        "# Compute ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y, y_score)  # Make sure 'thresholds' is defined!\n",
        "\n",
        "# Plot the ROC Curve\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.4f})', color='blue')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')  # Random model line\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:30:39.917808Z",
          "iopub.execute_input": "2025-02-26T20:30:39.918203Z",
          "iopub.status.idle": "2025-02-26T20:30:41.232879Z",
          "shell.execute_reply.started": "2025-02-26T20:30:39.918163Z",
          "shell.execute_reply": "2025-02-26T20:30:41.231557Z"
        },
        "id": "gUijWuJA3lA4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        ">\n",
        ">0.9 - 1.0 → Excellent."
      ],
      "metadata": {
        "id": "B5ssOR4E3lA4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.➡️Youden's J statistic\n",
        ">The optimal cutoff balances the True Positive Rate (TPR) and False Positive Rate (FPR). A common method is Youden’s J statistic:\n",
        ">\n",
        ">\\[\n",
        "J = TPR - FPR\n",
        "\\]\n"
      ],
      "metadata": {
        "id": "nQdC3vNR3lA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Youden's J statistic\n",
        "j_scores = tpr - fpr\n",
        "optimal_idx = np.argmax(j_scores)\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "\n",
        "print(f\"Optimal Cutoff Probability: {optimal_threshold:.4f}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:30:41.233893Z",
          "iopub.execute_input": "2025-02-26T20:30:41.234194Z",
          "iopub.status.idle": "2025-02-26T20:30:41.241203Z",
          "shell.execute_reply.started": "2025-02-26T20:30:41.234169Z",
          "shell.execute_reply": "2025-02-26T20:30:41.23996Z"
        },
        "id": "V2fNYRb-3lA5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Convert Probability Cutoff to Score Cutoff\n",
        "To determine the equivalent score cutoff, reverse the logistic function:\n",
        "\n",
        "$$\n",
        "\\[\n",
        "\\text{Score} = \\text{min\\_score} + (\\text{max\\_score} - \\text{min\\_score}) \\times \\log\\left(\\frac{p}{1-p}\\right)\n",
        "\\]\n",
        "$$"
      ],
      "metadata": {
        "id": "VYc1bWed3lA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert probability cutoff to score cutoff\n",
        "optimal_score_cutoff = min_score + (max_score - min_score) * np.log(optimal_threshold / (1 - optimal_threshold))\n",
        "\n",
        "print(f\"Optimal Score Cutoff: {optimal_score_cutoff:.0f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:30:41.242339Z",
          "iopub.execute_input": "2025-02-26T20:30:41.242684Z",
          "iopub.status.idle": "2025-02-26T20:30:41.265815Z",
          "shell.execute_reply.started": "2025-02-26T20:30:41.242654Z",
          "shell.execute_reply": "2025-02-26T20:30:41.264602Z"
        },
        "id": "3MxZroQS3lA5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Now, classify customers into \"Good\" or \"Bad\" risk segments using the optimal cutoff:"
      ],
      "metadata": {
        "id": "QuNZe1i13lA5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.📉Risk Category🔴🟢"
      ],
      "metadata": {
        "id": "MBcls8ru3lA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign risk category based on cutoff\n",
        "df_prep_scores['Risk_Category'] = np.where(df_prep_scores >= optimal_score_cutoff, \"Good\", \"Bad\")\n",
        "\n",
        "# View distribution\n",
        "df_prep_scores['Risk_Category'].value_counts()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-26T20:30:41.266953Z",
          "iopub.execute_input": "2025-02-26T20:30:41.26748Z",
          "iopub.status.idle": "2025-02-26T20:30:41.744795Z",
          "shell.execute_reply.started": "2025-02-26T20:30:41.267399Z",
          "shell.execute_reply": "2025-02-26T20:30:41.743599Z"
        },
        "id": "Fhgx7l4m3lA5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        ">\n",
        ">- Customers above the cutoff → \"Good\" Risk (Low probability of default).\n",
        ">- Customers below the cutoff → \"Bad\" Risk (High probability of default).\n"
      ],
      "metadata": {
        "id": "FvN1xHxF3lA6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🎯Next Steps\n",
        "\n",
        "> Here is direct link to next steps:\n",
        "> Complete Credit Risk Modeling  | 1. EDA <br>\n",
        "> direct link here: <br>\n",
        "https://www.kaggle.com/code/beatafaron/complete-credit-risk-modeling-1-eda <br>\n",
        "> 2. Behavioral Scorecards, weight of evidence, logistic regresion.\n",
        "> https://www.kaggle.com/code/beatafaron/complete-credit-risk-modeling-2-sc-woe\n",
        "> <br>\n",
        "> 3. Population stability index.<br>\n",
        "> https://www.kaggle.com/code/beatafaron/complete-credit-risk-modeling-3-psi"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-26T20:30:41.74596Z",
          "iopub.execute_input": "2025-02-26T20:30:41.746398Z",
          "iopub.status.idle": "2025-02-26T20:30:41.751016Z",
          "shell.execute_reply.started": "2025-02-26T20:30:41.746341Z",
          "shell.execute_reply": "2025-02-26T20:30:41.749841Z"
        },
        "id": "9Mtcx2cB3lA6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "jeBm88TQ3lA6"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}